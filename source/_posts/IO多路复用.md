---
title: IO多路复用
date: 2025-05-05 13:22:42
tags:
index_img: /images/iomulti.webp
banner_img: /images/iomulti.webp
---

# IO流程
![](/images/ioFLow.webp)
总的来说IO分两个阶段：
- 数据等待阶段：当进程或线程发起IO请求（如：调用 recvfrom 系统调用）时，它会一直阻塞，直到内核确认数据已准备好（例：网卡接收数据、网络数据到达内核缓冲区）。
- 数据复制阶段：内核将数据从内核空间复制到用户空间时，线程/进程仍处于阻塞状态。此过程线程/进程在等待I/O完成期间无法执行其他任务（被挂起），CPU资源可能闲置。
## 底层数据流
![](/images/fiveLayer.webp)
![](/images/fiveStruct.webp)
![](/images/packetFLow.webp)
### 到达网卡
数据包到达网卡之后，网卡会校验接收到的数据包中的目的 MAC 地址是不是自己的 MAC 地址，如果不是的话通常就会丢弃掉
这种只接受发送给自己的数据包（其余的扔掉）的工作模式称为非混杂模式（Non-Promiscuous Mode）
混杂模式（Promiscuous Mode）则是网卡会接收通过网络传输的所有数据包，而不仅仅是发送给它自己的数据包
非混杂模式是网卡默认的工作模式，可以尽可能的保护网络安全和减少网络负载
网卡在校验完 MAC 地址之后还会校验数据帧（Data Frame）中校验字段 FCS 来一次确保接收到的数据包是正确的
这些主要靠硬件完成，cpu
### 网卡硬件缓冲区 ——> 系统内存（ring buffer） 
当网卡接收到数据包时，它将数据包的内容存储在硬件缓冲区中，然后通过 DMA 将接收到的数据从硬件缓冲区传输到系统内存中的指定位置，这个位置通常是一个环形缓冲区（ ring buffer）
DMA（直接内存访问，Direct Memory Access） DMA是一种数据传输技术，允许外设（如网卡、硬盘控制器、显卡等）直接访问计算机内存，而无需经过 CPU 通过 DMA 可以大大提高数据传输的效率，减轻 CPU 的负担
### 触发硬中断
当网卡将数据包 DMA 到用于接收的环形缓冲区（rx_ring）之后，就会触发一个硬中断来告诉 CPU 数据包收到了
什么时候会触发一个硬中断，可以通过下面的参数来进行配置：
rx-usecs：当过这么长时间过后，一个中断就会被产生
rx-frames：当累计接收到这么多个数据帧后，一个中断就会被产生
当 ring buffer 满了之后，新来的数据包将给丢弃
ifconfig 查看网卡的时候，可以里面有个 overruns，表示因为环形队列满而被丢弃的包
CPU 收到硬中断之后就会停止手中的活，保存上下文，然后去调用网卡驱动注册的硬中断处理函数
为数据包分配 skb_buff ，当一个数据包经过了网卡引起中断之后，每一个包都会在内存中分配一块区域，称为 sk_buff (套接字缓存，socket buffer )
sk_buff  是 Linux 网络的一个核心数据结构
### 触发软中断
网卡的硬中断处理函数处理完之后驱动先 disable 硬中断，然后 enable 软中断
待 ring buffer 中的所有数据包被处理完成后，enable 网卡的硬中断，这样下次网卡再收到数据的时候就会通知 CPU
内核负责软中断进程 ksoftirqd 发现有软中断请求到来，进行下面的一些操作
- 调用 net_rx_action 函数
它会通过 poll 函数去 rx_ring 中拿数据帧，获取的时候顺便把 rx_ring 上的数据给删除,除此之外，poll 函数会把 ring buffer 中的数据包转换成内核网络模块能够识别的 skb 格式（即 socket kernel buffer）
- 最后进入 netif_receive_skb 处理流程，它是数据链路层接收数据帧的最后一关
根据注册在全局数组 ptype_all 和 ptype_base 里的网络层数据帧类型去调用第三层协议的接收函数处理
例如对于 ip 包来讲，就会进入到 ip_rcv；如果是 arp 包的话，会进入到 arp_rcv
# 阻塞IO
- 只有数据在用户空间准备就绪，此时进程才解除阻塞状态，阻塞状态，cpu不会分配时间片。
- 默认都是阻塞IO
- 同步调用并不是阻塞IO，同步调用会占用cpu，可以执行其他逻辑，会主动检测IO是否准备好。
- 在IO的两个阶段均会阻塞线程。
## 特点
1. 阻塞挂起： 进程/线程在等待数据时会被挂起，不占用 CPU 资源。
2. 及时响应： 每个操作都能得到及时处理，适合对实时性要求较高的场景。
3. 实现简单： 开发难度低，逻辑直观，代码按顺序执行，无需处理多线程或异步回调的复杂性。
4. 适用场景： 阻塞式 I/O 模型适合并发量较小、对实时性要求较高的应用。但在高并发场景中，其系统开销和性能限制使其不再适用。
5. 系统开销大：由于每个请求都会阻塞进程/线程，因此需要为每个请求分配独立的进程或线程来处理。在高并发场景下，这种模型会消耗大量系统资源（如内存和上下文切换开销），导致性能瓶颈。
# 非阻塞IO
![](/images/nonblockio.webp)
- 如果内核缓冲区没有数据，内核会立即返回一个错误（如 EWOULDBLOCK 或 EAGAIN），而不会阻塞进程。
- 如果内核缓冲区有数据，内核会将数据复制到用户空间并返回成功。
- 阻塞 IO 和非阻塞 IO 的区别就在于：应用程序的调用是否立即返回
## 特点
1. 非阻塞： 进程不会被挂起，无论是否有数据都会立即返回。
2. 轮询机制： 进程需要不断发起系统调用（轮询）来检查数据是否就绪，这会消耗大量 CPU 资源。
3. 实现难度较低： 相比阻塞式 I/O，开发复杂度稍高，但仍属于较简单的模型。
4. 实时性差： 轮询机制无法保证及时响应数据到达事件，可能导致延迟。
5. 适用场景： 适合并发量较小、且对实时性要求不高的网络应用开发。由于其 CPU 开销较大，通常不适用于高并发或高性能场景
# 信号驱动IO
![](/images/signalIO.webp)
在信号驱动 I/O 模型中，进程发起一个 I/O 操作时，会向内核注册一个信号处理函数（如 SIGIO），然后立即返回，不会被阻塞。当内核数据就绪时，会向进程发送一个信号，进程在信号处理函数中调用 I/O 操作（如 recvfrom）读取数据。
## 特点
1. 非阻塞： 进程在等待数据时不会被阻塞，可以继续执行其他任务。
2. 回调机制： 通过信号通知的方式实现异步事件处理，数据就绪时内核主动通知进程。
3. 实现难度大： 信号处理函数的编写和调试较为复杂，开发难度较高。
4. 信号处理复杂性： 信号处理函数需要处理异步事件，可能引入竞态条件和不可预测的行为。
5. 适用场景有限： 适合对实时性要求较高、但并发量较小的网络应用开发。由于其实现复杂性和潜在问题，通常不适用于高并发或高性能场景。
# 异步IO
![](/images/yiBuIO.webp)
- 在异步 I/O 模型中，当进程发起一个 I/O 操作时，会立即返回，不会被阻塞，也不会立即返回结果。内核会负责完成整个 I/O 操作（包括数据准备和复制到用户空间），并在操作完成后通知进程。如果 I/O 操作成功，进程可以直接获取到数据。
- 同步 IO 和异步 IO 的区别就在于：数据拷贝的时候进程是否阻塞
## 特点
1. 完全非阻塞： 进程在发起 I/O 操作后不会被阻塞，可以继续执行其他任务。
2. Proactor 模式： 内核负责完成 I/O 操作并通知进程，进程只需处理最终结果。
3. 高性能： 适合高并发、高性能场景，能够充分利用系统资源。
4. 操作系统支持： 异步 I/O 需要操作系统的底层支持。在 Linux 中，异步 I/O 从 2.5 版本内核开始引入，并在 2.6 版本中成为标准特性。
5. 实现难度大： 异步 I/O 的开发复杂度较高，需要处理回调、事件通知等机制。
6. 适用场景： 异步 I/O 模型非常适合高性能、高并发的网络应用开发，如大规模 Web 服务器、数据库系统等
# 多路复用
![](/images/fuyongio.webp)
- **多路:**是指多个网络连接(Socket)
- **复用:**是指通过一个线程同时监控多个文件描述符的就绪状态。这样，程序可以高效地处理多个 I/O 事件，而不需要为每个连接创建单独的线程，从而节省系统资源。
- 主要具体技术:epoll,poll,select(本质上都是同步IO)
- 与多进程和多线程技术相比，I/O多路复用技术的最大优势是系统开销小，系统不必创建进程/线程，也不必维护这些进程/线程，从而大大减小了系统的开销。 I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间
## select
![](/images/selectIO.gif)
### 缺点
- 每次调用select，都需要把被监控的fds集合从用户态空间拷贝到内核态空间，高并发场景下这样的拷贝会使得消耗的资源是很大的
- 能监听端口的数量有限，单个进程所能打开的最大连接数由FD_SETSIZE宏定义，监听上限就等于fds_bits位数组中所有元素的二进制位总数，其大小是32个整数的大小（在32位的机器上，大小就是3232，同理64位机器上为3264），当然我们可以对宏FD_SETSIZE进行修改，然后重新编译内核，但是性能可能会受到影响，一般该数和系统内存关系很大，具体数目可以cat /proc/sys/fs/file-max察看。32位机默认1024个，64位默认2048
- 被监控的fds集合中，只要有一个有数据可读，整个socket集合就会被遍历一次调用sk的poll函数收集可读事件：由于当初的需求是朴素，仅仅关心是否有数据可读这样一个事件，当事件通知来的时候，由于数据的到来是异步的，我们不知道事件来的时候，有多少个被监控的socket有数据可读了，于是，只能挨个遍历每个socket来收集可读事件了
## poll
- poll 的实现与 select 非常相似，都是通过监视多个文件描述符（fd）来实现 I/O 多路复用。两者的主要区别在于描述 fd 集合的方式：select 使用 fd_set 结构，而 poll 使用 pollfd 结构。select 的 fd_set 结构限制了 fd 集合的大小（通常为 1024），而 poll 使用 pollfd 结构，理论上可以支持更多的 fd，解决了 select 的问题 (2)。
- 与 select 类似，poll 也存在性能瓶颈。当监视的 fd 数量较多时，poll 需要将整个 pollfd 数组在用户态和内核态之间复制，无论这些 fd 是否就绪。这种复制的开销会随着 fd 数量的增加而线性增长，导致性能下降。
- poll 适合需要监视较多 fd 的场景，但在高并发或 fd 数量非常大的情况下，性能仍然不如 epoll。
- 从实现来看。很明显它并没优化大量描述符数组被整体复制于用户态和内核态的地址空间之间，以及个别描述符就绪触发整体描述符集合的遍历的低效问题。

## epoll
![](/images/epollIO.gif)
### epoll惊群
多个进程等待在ep->wq上，事件触发后所有进程都被唤醒，但只有其中1个进程能够成功继续执行的现象。
其他被白白唤起的进程等于做了无用功，可能会造成系统负载过高的问题。
为了解决epoll惊群，内核后续的高版本又提供了EPOLLEXCLUSIVE选项和SO_REUSEPORT选项，我个人理解两种解决方案思路上的不同点在于：EPOLLEXCLUSIVE是在唤起进程阶段起作用，只唤起排在队列最前面的1个进程；而SO_REUSEPORT是在分配连接时起作用，相当于每个进程自己都有一个独立的epoll实例，内核来决策把连接分配给哪个epoll。
### 水平触发(LT)和边缘触发(ET)
- 是epoll_wait的两种工作模式
#### 区别
- 客户端都是输入“abcdefgh” 8个字符，服务端每次接收2个字符。水平触发时，客户端输入8个字符触发了一次读就绪事件，由于被监视文件上还有数据可读故一直返回读就绪，服务端4次循环每次都能取到2个字符，直到8个字符全部读完
- 边缘触发时，客户端同样输入8个字符但服务端一次循环读到2个字符后这个读就绪事件就没有了。等客户端再输入一个字符串后，服务端关注到了数据的“变化”继续从缓冲区读接下来的2个字符“c”和”d”
### 相比select和poll的优点
#### 事件驱动机制(基于回调,而非轮询)
- select 和 poll 的轮询机制： select 和 poll 采用轮询的方式检查所有被监视的文件描述符（fd），无论这些 fd 是否就绪。每次调用时，都需要将整个 fd 集合从用户态复制到内核态，并在内核中遍历所有 fd 来检查其状态。随着 fd 数量的增加，轮询的开销会线性增长，导致性能显著下降。
- epoll 的事件驱动机制：- epoll 使用基于事件回调的机制。内核会维护一个就绪队列，只关注那些状态发生变化的 fd（即活跃的 fd）。一旦检测到epoll管理的socket描述符就绪时，内核会采用类似 callback 的回调机制，将其加入就绪队列，epoll_wait 只需从队列中获取就绪的 fd，而不需要遍历所有 fd。这种机制使得 epoll 的性能不会随着 fd 数量的增加而显著下降。
#### 避免频繁的用户态与内核态数据拷贝
- select 和 poll 的数据拷贝问题： 每次调用 select 或 poll 时，都需要将整个 fd 集合从用户态复制到内核态，调用结束后再将结果从内核态复制回用户态。这种频繁的数据拷贝在高并发场景下会带来较大的性能开销。
- epoll 的优化： epoll 使用了内存映射（ mmap ）技术，这样便彻底省掉了这些socket描述符在系统调用时拷贝的开销（因为从用户空间到内核空间需要拷贝操作）。mmap将用户空间的一块地址和内核空间的一块地址同时映射到相同的一块物理内存地址（不管是用户空间还是内核空间都是虚拟地址，最终要通过地址映射映射到物理地址），使得这块物理内存对内核和对用户均可见，减少用户态和内核态之间的数据交换，不需要依赖拷贝，这样子内核可以直接看到epoll监听的socket描述符，效率极高。
#### 支持更大的并发连接数
- select 的 fd 数量限制： select 使用 fd_set 结构，其大小通常被限制为 1024（由`__FD_SETSIZE`定义），这意味着它最多只能同时监视 1024 个 fd。虽然可以通过修改内核头文件并重新编译内核来扩大这一限制，但这并不能从根本上解决问题。
- poll 的改进与局限： poll 使用 pollfd 结构，理论上可以支持更多的 fd，但它仍然需要遍历所有 fd，性能会随着 fd 数量的增加而下降。
- epoll 的无限制支持： epoll 没有 fd 数量的硬性限制，适合高并发场景，能够轻松支持数万甚至数十万的并发连接。
# 参考链接
- https://weibo.com/ttarticle/p/show?id=2309404643915947966659
- https://mp.weixin.qq.com/s/26BkzSBHUZTVdgPF8IJRMQ?poc_token=HGNMGGij7DFKRrGtoT0BUEQfFP419iP6G76H6Ke4
- https://mp.weixin.qq.com/s?__biz=MzkzNzI1MzE2Mw==&mid=2247486122&idx=1&sn=df659a7458028772c9595e98d5cefbc1&chksm=c2930aeef5e483f843c282f43fcba02c919d773a33a1c54cbf2c5d6f1076500c7c2e27468176&token=198919963&lang=zh_CN#rd
